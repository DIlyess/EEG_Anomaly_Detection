{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated analysis of EEG quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's load the training data\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_PATH = Path(\"train\")\n",
    "training_data = [(np.load(ROOT_PATH / f\"data_{i}.npy\"),np.load(ROOT_PATH / f\"target_{i}.npy\")) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_array_into_windows(x, sample_rate, window_duration_in_seconds):\n",
    "    \"\"\"\n",
    "    Reshape the data into an array of shape (C, T, window) where 'window' contains\n",
    "    the points corresponding to 'window_duration' seconds of data.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy array): The input data array.\n",
    "    sample_rate (int): The number of samples per second.\n",
    "    window_duration_in_seconds (float): The duration of each window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    reshaped_x (numpy array): The reshaped array with shape (C, T, window).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples in one window\n",
    "    window_size = int(window_duration_in_seconds * sample_rate)\n",
    "    \n",
    "    # Ensure the total length of x is a multiple of window_size\n",
    "    total_samples = x.shape[-1]\n",
    "    if total_samples % window_size != 0:\n",
    "        # Truncate or pad x to make it divisible by window_size\n",
    "        x = x[..., :total_samples - (total_samples % window_size)]\n",
    "    # Reshape x into (C, T, window)\n",
    "    reshaped_x = x.reshape(x.shape[0], -1, window_size)\n",
    "\n",
    "    return reshaped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7712740)\n",
      "(5, 15425, 500)\n",
      "(77125,)\n",
      "(77125, 500)\n",
      "(5, 5232364)\n",
      "(5, 10464, 500)\n",
      "(52320,)\n",
      "(52320, 500)\n",
      "(5, 6421756)\n",
      "(5, 12843, 500)\n",
      "(64215,)\n",
      "(64215, 500)\n",
      "(5, 6809761)\n",
      "(5, 13619, 500)\n",
      "(68095,)\n",
      "(68095, 500)\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "all_targets = []\n",
    "for (data,target) in training_data:\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    print(filtered_data.shape)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "    print(reshaped_data.shape)\n",
    "    targets_flatten = target[..., :len(reshaped_data[0])].reshape(-1)\n",
    "    print(targets_flatten.shape)\n",
    "    reshaped_data = reshaped_data.reshape((-1,reshaped_data.shape[-1]))\n",
    "    print(reshaped_data.shape)\n",
    "    all_data.append(reshaped_data)\n",
    "    all_targets.append(targets_flatten)\n",
    "all_data = np.concatenate(all_data)\n",
    "all_targets = np.concatenate(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now compute the mean, max and stdev over each 2 seconds segment to try to build features\n",
    "amplitude = (np.max(all_data,-1) - np.min(all_data,-1)).reshape(-1)\n",
    "maxwindow=(np.max(all_data,-1)).reshape(-1)\n",
    "\n",
    "std = (np.std(all_data, axis=-1)).reshape(-1)\n",
    "#df[\"log_std\"] = np.log(std + 1)\n",
    "\n",
    "training_data_features = pd.DataFrame({\"amplitude\":amplitude, \"max\":maxwindow, \"std\":std, \"target\":all_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5936568717279334\n",
      "0.7331751824817518\n"
     ]
    }
   ],
   "source": [
    "# We train a model on 70% of the data and evaluate the model on the remaining 30%\n",
    "prop_train = 0.7\n",
    "n_train = int(prop_train * len(training_data_features))\n",
    "train_set = training_data_features[:n_train]\n",
    "val_set = training_data_features[n_train:]\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=8)\n",
    "neigh.fit(np.array(train_set[[\"amplitude\",\"std\"]]), train_set[\"target\"])\n",
    "prediction = neigh.predict(np.array(val_set[[\"amplitude\",\"std\"]]))\n",
    "\n",
    "\n",
    "\n",
    "print(cohen_kappa_score(prediction,val_set[\"target\"]))\n",
    "print(f1_score(val_set[\"target\"],prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342842408015843\n",
      "0.8914599365011031\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fraction pour l'entraînement\n",
    "prop_train = 0.7\n",
    "\n",
    "# Diviser les données de manière aléatoire en train et test\n",
    "train_set, val_set = train_test_split(training_data_features, test_size=1 - prop_train, random_state=42)\n",
    "\n",
    "# Initialiser le modèle KNN\n",
    "neigh = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# Entraîner le modèle avec les colonnes \"amplitude\" et \"std\"\n",
    "neigh.fit(np.array(train_set[[\"amplitude\", \"std\"]]), train_set[\"target\"])\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de validation\n",
    "prediction = neigh.predict(np.array(val_set[[\"amplitude\", \"std\"]]))\n",
    "\n",
    "\n",
    "\n",
    "print(cohen_kappa_score(prediction,val_set[\"target\"]))\n",
    "print(f1_score(val_set[\"target\"],prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5497107727642652\n",
      "0.7098133444769467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#with random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prop_train = 0.7\n",
    "n_train = int(prop_train * len(training_data_features))\n",
    "train_set = training_data_features[:n_train]\n",
    "val_set = training_data_features[n_train:]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=5, random_state=4)  # n_estimators = nombre d'arbres\n",
    "rf.fit(np.array(train_set[[\"amplitude\",\"std\"]]), train_set[\"target\"])\n",
    "prediction = rf.predict(np.array(val_set[[\"amplitude\",\"std\"]]))\n",
    "\n",
    "\n",
    "print(cohen_kappa_score(prediction,val_set[\"target\"]))\n",
    "print(f1_score(val_set[\"target\"],prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant tester de prendre en compte les 5 channels simultanément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7712740)\n",
      "(5, 15425, 500)\n",
      "(15425, 5, 500)\n",
      "(15425, 5)\n",
      "(5, 5232364)\n",
      "(5, 10464, 500)\n",
      "(10464, 5, 500)\n",
      "(10464, 5)\n",
      "(5, 6421756)\n",
      "(5, 12843, 500)\n",
      "(12843, 5, 500)\n",
      "(12843, 5)\n",
      "(5, 6809761)\n",
      "(5, 13619, 500)\n",
      "(13619, 5, 500)\n",
      "(13619, 5)\n",
      "(52351, 5)\n"
     ]
    }
   ],
   "source": [
    "all_data_bychannels = []\n",
    "all_targets_bychannels = []\n",
    "for (data,target) in training_data:\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    print(filtered_data.shape)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "    print(reshaped_data.shape)\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    print(reshaped_data.shape)\n",
    "    targets_flatten = target.transpose(1,0)\n",
    "    print(targets_flatten.shape)\n",
    "    all_data_bychannels.append(reshaped_data)\n",
    "    all_targets_bychannels.append(targets_flatten)\n",
    "all_data_bychannels = np.concatenate(all_data_bychannels)\n",
    "all_targets_bychannels = np.concatenate(all_targets_bychannels)\n",
    "\n",
    "print(all_targets_bychannels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52351, 3)\n"
     ]
    }
   ],
   "source": [
    "amplitude = []\n",
    "std = []\n",
    "targets = []\n",
    "\n",
    "block_size = 5  # Taille des blocs\n",
    "\n",
    "# Parcourir les canaux de données\n",
    "for channel_idx, channel_data in enumerate(all_data_bychannels):\n",
    "    # Parcourir les segments de ce canal par blocs de 5\n",
    "    for i in range(0, len(channel_data), block_size):\n",
    "        block = channel_data[i:i + block_size]\n",
    "        if len(block) == block_size:  # S'assurer que le bloc a la bonne taille\n",
    "            # Calculer les caractéristiques pour chaque segment du bloc\n",
    "            amp_block = [np.max(seg) - np.min(seg) for seg in block]\n",
    "            std_block = [np.std(seg) for seg in block]\n",
    "            \n",
    "            # Ajouter les caractéristiques du bloc\n",
    "            amplitude.append(amp_block)  # Liste des amplitudes des 5 segments\n",
    "            std.append(std_block)  # Liste des écarts-types des 5 segments\n",
    "            \n",
    "            # Ajouter la cible correspondante (liste de 5 éléments)\n",
    "            targets.append(all_targets_bychannels[channel_idx][i:i + block_size])\n",
    "\n",
    "# Construire le DataFrame\n",
    "training_data_features = pd.DataFrame({\n",
    "    \"amplitude\": amplitude,\n",
    "    \"std\": std,\n",
    "    \"target\": targets\n",
    "})\n",
    "\n",
    "print(training_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa Score: 0.8301405279671754\n",
      "F1 Score: 0.9323029693537289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de validation\n",
    "train_set, val_set = train_test_split(\n",
    "    training_data_features, \n",
    "    test_size=0.3,  # 30% pour la validation\n",
    "    random_state=42,  # Pour la reproductibilité\n",
    "    shuffle=True  # Mélanger les données avant de les diviser\n",
    ")\n",
    "\n",
    "# Transformer les blocs de 5 en un tableau 2D pour les caractéristiques\n",
    "X_train = np.hstack([\n",
    "    np.stack(train_set[\"amplitude\"].to_list()),  # Transforme chaque liste de 5 amplitudes en colonnes\n",
    "    np.stack(train_set[\"std\"].to_list())         # Transforme chaque liste de 5 std en colonnes\n",
    "])\n",
    "y_train = np.array(train_set[\"target\"].to_list())  # Chaque cible reste un bloc de 5\n",
    "\n",
    "X_val = np.hstack([\n",
    "    np.stack(val_set[\"amplitude\"].to_list()),\n",
    "    np.stack(val_set[\"std\"].to_list())\n",
    "])\n",
    "y_val = np.array(val_set[\"target\"].to_list())\n",
    "\n",
    "# Entraîner un modèle\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "predictions = rf.predict(X_val)\n",
    "\n",
    "# Aplatir les cibles et les prédictions pour les métriques\n",
    "y_val_flatten = y_val.flatten()\n",
    "predictions_flatten = predictions.flatten()\n",
    "\n",
    "# Calculer les scores\n",
    "kappa = cohen_kappa_score(predictions_flatten, y_val_flatten)\n",
    "f1 = f1_score(y_val_flatten, predictions_flatten)\n",
    "\n",
    "print(f\"Cohen Kappa Score: {kappa}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fait d'avoir pris en compte l'interaction entre les channels a fait augmenter le score Cohen Kappa.\n",
    "Nous allons maintenant rajouter plusieurs features pour essayer d'améliorer notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_prominences\n",
    "\n",
    "\n",
    "def max_peak_prominence(x):\n",
    "    peaks, _ = find_peaks(x)\n",
    "    if len(peaks) == 0:\n",
    "        return 0\n",
    "    prominences = peak_prominences(x, peaks)[0]\n",
    "    return np.max(prominences)\n",
    "\n",
    "def count_peaks(x):\n",
    "    return len(find_peaks(x)[0])\n",
    "\n",
    "amplitude = []\n",
    "std = []\n",
    "MMD = []\n",
    "energy = []\n",
    "max_peak_prom = []\n",
    "number_peaks = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "\n",
    "block_size = 5  # Taille des blocs\n",
    "\n",
    "# Parcourir les canaux de données\n",
    "for channel_idx, channel_data in enumerate(all_data_bychannels):\n",
    "    # Parcourir les segments de ce canal par blocs de 5\n",
    "    for i in range(0, len(channel_data), block_size):\n",
    "        block = channel_data[i:i + block_size]\n",
    "        if len(block) == block_size:  # S'assurer que le bloc a la bonne taille\n",
    "            # Calculer les caractéristiques pour chaque segment du bloc\n",
    "            amp_block = [np.max(seg) - np.min(seg) for seg in block]\n",
    "            std_block = [np.std(seg) for seg in block]\n",
    "            MMD_block = [(np.argmax(seg)-np.argmin(seg))**2+(np.max(seg)-np.min(seg))**2 for seg in block]\n",
    "            energy_block = [np.sum(seg**2) for seg in block]\n",
    "            max_peak_prom_block = [max_peak_prominence(seg) for seg in block]\n",
    "            number_peaks_block = [count_peaks(seg) for seg in block]\n",
    "\n",
    "            # Ajouter les caractéristiques du bloc\n",
    "            amplitude.append(amp_block)  # Liste des amplitudes des 5 segments\n",
    "            std.append(std_block)  # Liste des écarts-types des 5 segments\n",
    "            MMD.append(MMD_block)\n",
    "            energy.append(energy_block)\n",
    "            max_peak_prom.append(max_peak_prom_block)\n",
    "            number_peaks.append(number_peaks_block)\n",
    "            \n",
    "            # Ajouter la cible correspondante (liste de 5 éléments)\n",
    "            targets.append(all_targets_bychannels[channel_idx][i:i + block_size])\n",
    "\n",
    "# Construire le DataFrame\n",
    "training_data_features = pd.DataFrame({\n",
    "    \"amplitude\": amplitude,\n",
    "    \"std\": std,\n",
    "    \"MMD\": MMD,\n",
    "    \"energy\": energy,\n",
    "    \"max_peak_prom\": max_peak_prom,\n",
    "    \"number_peaks\": number_peaks,\n",
    "    \"target\": targets\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15706, 5)\n",
      "(78530,)\n",
      "Cohen Kappa Score: 0.9175809912163921\n",
      "F1 Score: 0.9672132880811156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de validation\n",
    "train_set, val_set = train_test_split(\n",
    "    training_data_features, \n",
    "    test_size=0.3,  # 30% pour la validation\n",
    "    random_state=42,  # Pour la reproductibilité\n",
    "    shuffle=True  # Mélanger les données avant de les diviser\n",
    ")\n",
    "\n",
    "# Transformer les blocs de 5 en un tableau 2D pour les caractéristiques\n",
    "X_train = np.hstack([\n",
    "    np.stack(train_set[\"amplitude\"].to_list()),  # Transforme chaque liste de 5 amplitudes en colonnes\n",
    "    np.stack(train_set[\"std\"].to_list()),         # Transforme chaque liste de 5 std en colonnes\n",
    "    #np.stack(train_set[\"MMD\"].to_list()),\n",
    "    np.stack(train_set[\"energy\"].to_list()),\n",
    "    np.stack(train_set[\"max_peak_prom\"].to_list()),\n",
    "    np.stack(train_set[\"number_peaks\"].to_list())\n",
    "])\n",
    "y_train = np.array(train_set[\"target\"].to_list())  # Chaque cible reste un bloc de 5\n",
    "\n",
    "X_val = np.hstack([\n",
    "    np.stack(val_set[\"amplitude\"].to_list()),\n",
    "    np.stack(val_set[\"std\"].to_list()),\n",
    "    #np.stack(val_set[\"MMD\"].to_list()),\n",
    "    np.stack(val_set[\"energy\"].to_list()),\n",
    "    np.stack(val_set[\"max_peak_prom\"].to_list()),\n",
    "    np.stack(val_set[\"number_peaks\"].to_list())\n",
    "])\n",
    "y_val = np.array(val_set[\"target\"].to_list())\n",
    "\n",
    "# Entraîner un modèle\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "predictions = rf.predict(X_val)\n",
    "print(predictions.shape)\n",
    "\n",
    "# Aplatir les cibles et les prédictions pour les métriques\n",
    "y_val_flatten = y_val.flatten()\n",
    "predictions_flatten = predictions.flatten()\n",
    "print(predictions_flatten.shape)\n",
    "\n",
    "# Calculer les scores\n",
    "kappa = cohen_kappa_score(predictions_flatten, y_val_flatten)\n",
    "f1 = f1_score(y_val_flatten, predictions_flatten)\n",
    "\n",
    "print(f\"Cohen Kappa Score: {kappa}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (36645, 25)\n",
      "Shape of y_train: (36645, 5)\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best parameters found: {'estimator__max_depth': 20, 'estimator__n_estimators': 100}\n",
      "Best score (F1 micro): 0.9651361316262658\n",
      "Shape of predictions: (15706, 5)\n",
      "F1 Score on validation set (micro): 0.966914875613471\n",
      "F1 Score on validation set (macro): 0.9614214792039208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de validation\n",
    "train_set, val_set = train_test_split(\n",
    "    training_data_features, \n",
    "    test_size=0.3,  # 30% pour la validation\n",
    "    random_state=42,  # Pour la reproductibilité\n",
    "    shuffle=True  # Mélanger les données avant de les diviser\n",
    ")\n",
    "\n",
    "# Transformer les blocs de 5 en un tableau 2D pour les caractéristiques\n",
    "X_train = np.hstack([\n",
    "    np.stack(train_set[\"amplitude\"].to_list()),  # Transforme chaque liste de 5 amplitudes en colonnes\n",
    "    np.stack(train_set[\"std\"].to_list()),         # Transforme chaque liste de 5 std en colonnes\n",
    "    #np.stack(train_set[\"MMD\"].to_list()),\n",
    "    np.stack(train_set[\"energy\"].to_list()),\n",
    "    np.stack(train_set[\"max_peak_prom\"].to_list()),\n",
    "    np.stack(train_set[\"number_peaks\"].to_list())\n",
    "])\n",
    "y_train = np.array(train_set[\"target\"].to_list())  # Chaque cible reste un bloc de 5\n",
    "\n",
    "X_val = np.hstack([\n",
    "    np.stack(val_set[\"amplitude\"].to_list()),\n",
    "    np.stack(val_set[\"std\"].to_list()),\n",
    "    #np.stack(val_set[\"MMD\"].to_list()),\n",
    "    np.stack(val_set[\"energy\"].to_list()),\n",
    "    np.stack(val_set[\"max_peak_prom\"].to_list()),\n",
    "    np.stack(val_set[\"number_peaks\"].to_list())\n",
    "])\n",
    "y_val = np.array(val_set[\"target\"].to_list())\n",
    "\n",
    "\n",
    "\n",
    "# Vérifier les dimensions des données\n",
    "print(\"Shape of X_train:\", X_train.shape)  # Exemple attendu : (36645, 25)\n",
    "print(\"Shape of y_train:\", y_train.shape)  # Exemple attendu : (36645, 5)\n",
    "\n",
    "# Création du classificateur multi-sortie\n",
    "multi_rf = MultiOutputClassifier(RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Définition des paramètres pour GridSearch\n",
    "param_grid = {\n",
    "    \"estimator__n_estimators\": [10, 50, 100],\n",
    "    \"estimator__max_depth\": [None, 10, 20],\n",
    "}\n",
    "\n",
    "# Scorer basé sur F1-Score (calculé pour chaque label individuellement, puis moyenné)\n",
    "scorer = make_scorer(f1_score, average=\"micro\")\n",
    "\n",
    "# Configuration de GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    multi_rf, \n",
    "    param_grid, \n",
    "    scoring=scorer, \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,  # Affiche les informations d'entraînement\n",
    "    n_jobs=-1   # Utilise tous les cœurs disponibles\n",
    ")\n",
    "\n",
    "# Entraîner avec GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le score associé\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best score (F1 micro):\", grid_search.best_score_)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de validation\n",
    "predictions = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "# Vérifier les dimensions des prédictions\n",
    "print(\"Shape of predictions:\", predictions.shape)  # Exemple attendu : (nb_samples, 5)\n",
    "\n",
    "# Calculer le F1-Score pour validation\n",
    "f1_micro = f1_score(y_val, predictions, average=\"micro\")\n",
    "print(\"F1 Score on validation set (micro):\", f1_micro)\n",
    "\n",
    "f1_macro = f1_score(y_val, predictions, average=\"macro\")\n",
    "print(\"F1 Score on validation set (macro):\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cohen Kappa Score: 0.9164015219254372\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Visualiser l'importance des caractéristiques\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43mrf_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n\u001b[0;32m     21\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(importances)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:673\u001b[0m, in \u001b[0;36mBaseForest.feature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;124;03m    The impurity-based feature importances.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;124;03m        array of zeros.\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     all_importances \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    676\u001b[0m         delayed(\u001b[38;5;28mgetattr\u001b[39m)(tree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_importances:\n",
      "File \u001b[1;32mc:\\Users\\Alexandre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle avec les meilleurs paramètres\n",
    "rf_final = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_rf_final = MultiOutputClassifier(rf_final)\n",
    "\n",
    "# Entraîner sur l'ensemble d'entraînement complet\n",
    "multi_rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de validation\n",
    "predictions_final = multi_rf_final.predict(X_val)\n",
    "\n",
    "# Calculer le Cohen Kappa Score final\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "kappa_final = cohen_kappa_score(y_val.flatten(), predictions_final.flatten())\n",
    "\n",
    "print(f\"Final Cohen Kappa Score: {kappa_final}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualiser l'importance des caractéristiques\n",
    "importances = rf_final.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices, rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le score est satisfaisant, il faut créer les fichiers csv pour les données de test et la soumission pour le leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_4: (5, 6602015)\n",
      "Shape of data_5: (5, 4659937)\n"
     ]
    }
   ],
   "source": [
    "ROOT_TEST_PATH = Path(\"test/\")\n",
    "test_data = {i:np.load(ROOT_TEST_PATH / f\"data_{i}.npy\") for i in [4,5]}\n",
    "for key, value in test_data.items():\n",
    "    print(f\"Shape of data_{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of preds: (5, 13204)\n",
      "Shape of preds: (5, 9319)\n"
     ]
    }
   ],
   "source": [
    "# We process each record independantly\n",
    "\n",
    "def compute_features_on_record(data):\n",
    "    \"\"\"\n",
    "    We compute each of the feature for each window and each channel\n",
    "    Each value of the output dict has shape (Channels,T)\n",
    "    \"\"\"\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    amplitude = []\n",
    "    std = []\n",
    "    energy = []\n",
    "    max_peak_prom = []\n",
    "    number_peaks = []\n",
    "\n",
    "    block_size = 5  # Taille des blocs\n",
    "\n",
    "    # Parcourir les canaux de données\n",
    "    for channel_idx, channel_data in enumerate(reshaped_data):\n",
    "        # Parcourir les segments de ce canal par blocs de 5\n",
    "        for i in range(0, len(channel_data), block_size):\n",
    "            block = channel_data[i:i + block_size]\n",
    "            if len(block) == block_size:  # S'assurer que le bloc a la bonne taille\n",
    "                # Calculer les caractéristiques pour chaque segment du bloc\n",
    "                amp_block = [np.max(seg) - np.min(seg) for seg in block]\n",
    "                std_block = [np.std(seg) for seg in block]\n",
    "                energy_block = [np.sum(seg**2) for seg in block]\n",
    "                max_peak_prom_block = [max_peak_prominence(seg) for seg in block]\n",
    "                number_peaks_block = [count_peaks(seg) for seg in block]\n",
    "\n",
    "                # Ajouter les caractéristiques du bloc\n",
    "                amplitude.append(amp_block)  # Liste des amplitudes des 5 segments\n",
    "                std.append(std_block)  # Liste des écarts-types des 5 segments\n",
    "                energy.append(energy_block)\n",
    "                max_peak_prom.append(max_peak_prom_block)\n",
    "                number_peaks.append(number_peaks_block)\n",
    "\n",
    "    return pd.DataFrame({\"amplitude\":amplitude,\n",
    "            \"std\": std,\n",
    "            \"energy\": energy,\n",
    "            \"max_peak_prom\": max_peak_prom,\n",
    "            \"number_peaks\": number_peaks})\n",
    "\n",
    "def compute_predictions_on_record(data,model,features_name_for_model):\n",
    "\n",
    "    features = compute_features_on_record(data)\n",
    "    \n",
    "\n",
    "    X_test = np.hstack([\n",
    "        np.stack(features[\"amplitude\"].to_list()),\n",
    "        np.stack(features[\"std\"].to_list()),\n",
    "        np.stack(features[\"energy\"].to_list()),\n",
    "        np.stack(features[\"max_peak_prom\"].to_list()),\n",
    "        np.stack(features[\"number_peaks\"].to_list())\n",
    "    ])\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = predictions.T\n",
    "    return predictions\n",
    "\n",
    "def format_array_to_target_format(array, record_number):\n",
    "    assert isinstance(record_number, int)\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert len(array.shape) == 2\n",
    "    assert array.shape[0] == 5\n",
    "    assert set(np.unique(array)) == {0, 1}\n",
    "    formatted_target = []\n",
    "    for i in range(array.shape[0]):\n",
    "        channel_encoding = (i + 1) * 100000\n",
    "        record_number_encoding = record_number * 1000000\n",
    "        for j in range(array.shape[1]):\n",
    "            formatted_target.append(\n",
    "                {\n",
    "                    \"identifier\": record_number_encoding + channel_encoding + j,\n",
    "                    \"target\": array[i, j],\n",
    "                }\n",
    "            )\n",
    "    return formatted_target\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for record_number, data in test_data.items():\n",
    "    preds = compute_predictions_on_record(data,rf,[\"amplitude\",\"std\",\"energy\",\"max_peak_prom\",\"number_peaks\"])\n",
    "    print(f\"Shape of preds: {preds.shape}\")\n",
    "    formatted_preds = format_array_to_target_format(preds,record_number)\n",
    "    results.extend(formatted_preds)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"submission_nesti5.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
